# ğŸ¦ Credit Risk ETL & Streaming Pipeline

This project is a structured ETL + Real-Time Streaming workflow that starts with a **Credit Risk CSV file** and ends in a **Kafka-powered data stream** with future PySpark transformations.

---

## ğŸ§± Project Structure

### ğŸ“ Phase 1: ETL (CSV to MySQL)
- **Extract**: Raw `credit_risk_data.csv`
- **Transform**: Clean, normalize data (e.g., rename columns, drop NA, convert types)
- **Load**: Insert cleaned data into MySQL table

âœ… **Tools**: Python (Pandas, SQLAlchemy), MySQL

```bash
ğŸ“‚ Testingcompose/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Credit Risk Benchmark Dataset.csv
â”œâ”€â”€ etl/
â”‚   â””â”€â”€ extract.py
â”‚   â””â”€â”€ load.py
â”‚   â””â”€â”€ transform.py
â”‚   â””â”€â”€ sqlcredcheck.py
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml  # Nothing till now 
â”œâ”€â”€ .env
|â”€â”€ venv
â”œâ”€â”€ main.py
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
â””â”€â”€ requirement.txt
â””â”€â”€ transform.log

---

### ğŸ“¡ Phase 2: Kafka & Debezium CDC
- Connect **MySQL** to **Kafka** using **Debezium Connector**
- Enable real-time Change Data Capture (CDC)
- Debezium publishes changes to **Kafka topics**

âœ… **Tools**: Kafka, Debezium, Kafka Connect

---

### ğŸ” Phase 3: Stream Transformations

#### 1ï¸âƒ£ Mini Project: Basic Kafka Consumer Transformation
- Consume Kafka topic
- Apply basic filtering or transformation logic
- Write to another topic or log output

#### 2ï¸âƒ£ Full Project: PySpark Streaming
- Use PySpark Structured Streaming to read from Kafka
- Apply advanced transformations (e.g., filtering, joining, aggregations)
- Optionally write transformed data to:
  - Another Kafka topic
  - HDFS
  - MySQL/Postgres

âœ… **Tools**: PySpark, Kafka, MySQL

---

## ğŸ§ª Folder Layout

```bash
ğŸ“‚ Testingcompose/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ credit_risk_data.csv
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ etl_to_mysql.py
â”‚   â””â”€â”€ kafka_transform.py
â”‚   â””â”€â”€ pyspark_transform.py
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml  # Kafka + Zookeeper + Debezium + MySQL
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ğŸ“ˆ Future Goals

    âœ… Build Apache Superset Dashboard for MySQL + Transformed Kafka Topics

    âœ… Airflow DAGs for automated ETL & Streaming

    âœ… Performance benchmarking between PySpark and Flink

ğŸ§  Author

Khizer Nayak
Fintech & Data Engineering Intern | Kafka + PySpark Enthusiast
ğŸŒ Exploring the world and data at the same time!